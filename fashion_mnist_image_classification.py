# -*- coding: utf-8 -*-
"""Fashion_MNIST_Image_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15iHw1CHiPF8fcnM6Vy0c09Zstnpzj2xg

# **Fashion MNIST Image Classification**

In this practical application notebook, we will work with the Fashion MNIST dataset to carry out a classification exercise using Artificial Neural Networks (ANNs). This project aims to provide a hands-on experience with image classification tasks, leveraging deep learning techniques.

## **About the dataset**

The dataset, Fashion MNIST, is a collection of 70,000 grayscale images of apparel items, categorized into 10 classes. These classes are numbered from 0 to 9 and have the following meanings:

| Class Number | Class Name     |
|--------------|----------------|
| 0            | T-shirt/top    |
| 1            | Trouser        |
| 2            | Pullover       |
| 3            | Dress          |
| 4            | Coat           |
| 5            | Sandal         |
| 6            | Shirt          |
| 7            | Sneaker        |
| 8            | Bag            |
| 9            | Ankle boot     |

Each image is 28x28 pixels, and the dataset is split into 60,000 training images and 10,000 test images. This dataset serves as a more challenging alternative to the original MNIST dataset, which contains handwritten digits.

## **Objective**

The objective of this project is to build a simple ANN model to classify the images into their respective categories. We will preprocess the data, construct and train the neural network, and evaluate its performance. By the end of this exercise, you will gain a solid understanding of the workflow involved in developing an image classification model.

## **Toolkit**

We will use the TensorFlow implementation of Keras for building and training our model. This project will be executed on **Google Colab**, which provides a convenient and powerful environment for running deep learning experiments.

---

**Author:** Sourena Mohit  
**Source:** This practical project comes from MIT IDSS

## **Importing the necessary libraries**
"""

import warnings
warnings.filterwarnings("ignore")

import tensorflow as tf

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

# Checking the version of Tensorflow

tf.__version__

"""## **Collecting the Data**

Let's import the data from the tf.keras.datasets module and prepare the train and the test sets.
"""

# Load the data

(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()

X_train.shape, X_test.shape

Y_train.shape, Y_test.shape

X_train.shape[1] * X_train.shape[2]

np.unique(Y_train)

"""## **Observations**


*   The Fashion MNIST dataset consists of **60,000 images of size 28x28 pixels** in the
training set and **10,000 images** of size 28x28 pixels in the test set. Each image represents an item of clothing from one of 10 classes.
*   Before fitting an ANN model, we need to ***flatten*** these images into a **one-dimensional array**. This transformation is necessary because the input layer of an ANN expects a flat vector rather than a 2D matrix.
*   This suggests that the training set has **10 classes**, where each class denotes a specific type of apparel.
*   By understanding the structure and composition of the dataset, we can better prepare our model for accurate classification.

## **Data Preparation**

To prepare our data for training the ANN model, we need to convert the labels into a categorical format and normalize the pixel values of the images.

First, we convert the labels using `to_categorical`:
"""

y_train = tf.keras.utils.to_categorical(Y_train, num_classes = 10)

y_test = tf.keras.utils.to_categorical(Y_test, num_classes = 10)

# Let's have a look at the shapes of all the sets

X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Let's normalize the dataset. Since there are pixel values ranging from 0-255, we divide by 255 to get the new ranges from 0-1

X_train = X_train/255

X_test = X_test/255

"""### **Visualization**
- Now, let us visualize the data items.
- We will visualize the first 24 images in the training dataset
"""

class_names_list = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize = (8, 8))

for i in range(24):

    plt.subplot(4, 6, i + 1)

    plt.xticks([])

    plt.yticks([])

    plt.grid(False)

    plt.imshow(X_train[i], cmap = plt.cm.binary)

    plt.xlabel(class_names_list[Y_train[i]])

plt.show()

"""## **Model Building**

- We will now start with the model building process.
- We will first create a model with:
 - A layer to flatten the input
 - A hidden layer with 64 nodes (you can play around with this number) and the 'relu' activation
 - Output layer with 10 nodes

- We will use the Adam optimizer, CategoricalCrossentropy as the loss, and accuracy as the metric.
"""

# Initialize a sequential model

model_1 = tf.keras.Sequential([

    tf.keras.layers.Flatten(input_shape = (28, 28)),

    tf.keras.layers.Dense(64, activation = 'relu'),

    tf.keras.layers.Dense(10, activation = 'softmax')
])

model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy',  metrics = ['accuracy'])

# Let us now fit the model

fit_history = model_1.fit(X_train, y_train,validation_split = 0.1, verbose = 1, epochs = 10, batch_size = 64)

"""**Observations:**
- We can observe that the model's accuracy increases with the increase in the number of epochs.
- For 10 epochs, we are getting an accuracy of about 90% for the training data and about 88% for the validation data.
"""

model_1.summary()

"""**Observations:**
- The summary of the model shows each layer's name, type, output shape, and the number of parameters at that particular layer.
- It also shows the total number of trainable and non-trainable parameters in the model. When a parameter is learned during training, it is called a trainable parameter, otherwise, it is called a non-trainable parameter.
- The Flatten layer simply flattens each image into a size of 784 (28*28) and there is no learning or training at this layer. Hence, the number of parameters is 0 for the Flatten layer.
- Each image in the form of 784 nodes would be the input for the 'dense' layer. Each node of the previous layer would be connected with each node of the current layer. Also, each connection has one weight to learn and each node has one bias. So, the total number of parameters are (784*64)+64 = 50,240.
- Similarly, the last layer - 'dense_1' has (64*10)+10 = 650 parameters.

### **Evaluate the model on the test set**
- Let's predict based on the test data. The .predict() method in Keras models returns the probabilities of each observation belonging to each class. We will choose the class with the highest predicted probability.
- Let's also create a function to print the classification report and confusion matrix.
"""

def metrics_score(actual, predicted):

    from sklearn.metrics import classification_report

    from sklearn.metrics import confusion_matrix

    print(classification_report(actual, predicted))

    cm = confusion_matrix(actual, predicted)

    plt.figure(figsize = (8, 5))

    sns.heatmap(cm, annot = True,  fmt = '.0f', xticklabels = class_names_list, yticklabels = class_names_list)

    plt.ylabel('Actual')

    plt.xlabel('Predicted')

    plt.show()

model_1.evaluate(X_test, y_test, verbose = 1)

test_pred = np.argmax(model_1.predict(X_test), axis = -1)

test_pred

metrics_score(Y_test, test_pred)

"""**Observations:**
- For each class, a classification report shows the classification metrics - precision, recall, f1-score.
- We can see that class 1 (Trousers), class 7 (Sneaker), class 8 (Bag), class 9 (Ankle boot) have the highest f1-score. Even such a simple model can identify these objects quite appropriately.
- Class 6 (Shirt) has the lowest recall. The model is not able to identify the shirt. The confusion matrix shows that the model has predicted shirts as T-shirts/tops, Pullover, and Coat which is understandable as these items have similar looks.
- Let's try changing the learning rate and train the model for more epochs and see if the model can identify subtle differences in different objects.

### **Further Iterations to model building**
- Let's change the learning rate and epochs and observe the effect on accuracy on the earlier network.
- Let's build a bigger network with the new learning rate and epochs.
"""

# Initialize sequential model

model_2 = tf.keras.Sequential([

    tf.keras.layers.Flatten(input_shape = (28, 28)),

    tf.keras.layers.Dense(64, activation = 'relu'),

    tf.keras.layers.Dense(10, activation = 'softmax')
])

model_2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

fit_history_2 = model_2.fit(X_train, y_train, epochs = 30, validation_split = 0.1, batch_size = 64, verbose = 2)

"""**Observations:**
- We can see that the accuracy of the training data has increased slightly but the increase in accuracy for the validation set is very less in comparison to the training data.
- This indicates that if we further increase the number of epochs while keeping everything else the same then the model might start to overfit.
"""

model_2.summary()

"""- The summary remains the same as the previous model because we have not changed anything about the structure of the NN.

Now, let's build another model with more nodes and layers.

#### Now, let's add more nodes to the layer and add another hidden layer
"""

# Initialize sequential model

model_3 = tf.keras.Sequential([

    tf.keras.layers.Flatten(input_shape = (28, 28)),

    tf.keras.layers.Dense(128, activation = 'relu'),

    tf.keras.layers.Dense(64, activation = 'relu'),

    tf.keras.layers.Dense(10, activation = 'softmax')
])

model_3.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

fit_history_3 = model_3.fit(X_train, y_train, epochs = 30, validation_split = 0.1, batch_size = 64, verbose = 1)

"""**Observations:**
- The validation accuracy of the model has further increased slightly, and the training accuracy has further increased by ~5%. So, there is still a hint of overfitting.
- We can play around with hyperparameters of the model or try different layer structures to improve the model performance and/or reduce the overfitting.
"""

model_3.summary()

"""**Observations:**
- We can see that the number of parameters has increased by ~2.15 times the number of parameters in previous models.
- Increasing the number of parameters can significantly increase the training time of the model.

#### Visualizing the loss and the accuracy on the train and the validation data for all three models
"""

dict_hist = fit_history.history

dict_hist_2 = fit_history_2.history

dict_hist_3 = fit_history_3.history

list_ep = [i for i in range(1, 11)]

list_ep30 = [i for i in range(1, 31)]


plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy_1')

plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy_1')

plt.plot(list_ep30, dict_hist_2['accuracy'], label = 'accuracy_2')

plt.plot(list_ep30, dict_hist_2['val_accuracy'], label = 'val_accuracy_2')

plt.plot(list_ep30, dict_hist_3['accuracy'], label = 'accuracy_3')

plt.plot(list_ep30, dict_hist_3['val_accuracy'], label = 'val_accuracy_3')

plt.ylabel('Accuracy')

plt.xlabel('Epochs')

plt.legend()

plt.show()

plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['loss'], ls = '-.', label = 'loss_1')

plt.plot(list_ep, dict_hist['val_loss'], ls = '-.', label = 'val_loss_1')

plt.plot(list_ep30, dict_hist_2['loss'], label = 'loss_2')

plt.plot(list_ep30, dict_hist_2['val_loss'], label = 'val_loss_2')

plt.plot(list_ep30, dict_hist_3['loss'], label = 'loss_3')

plt.plot(list_ep30, dict_hist_3['val_loss'], label = 'val_loss_3')

plt.ylabel('Loss')

plt.xlabel('Epochs')

plt.legend()

plt.show()

"""**Observations:**
- We can see that accuracy keeps increasing for the test data as the number of epochs increased but validation accuracy has become somewhat constant after 10 epochs.
- This indicates that the model learns the training data more closely after each epoch but cannot replicate the performance on the validation data, which is a sign of overfitting.
- The same pattern can be observed for loss as well. It keeps decreasing for the training data with the increase in epochs but becomes somewhat constant for the validation data after 10 epochs.

Now, let's make final predictions on the test data using the last model we built.

## **Final Predictions on the Test Data**
"""

final_pred = np.argmax(model_3.predict(X_test), axis  = -1)

metrics_score(Y_test, final_pred)

"""**Observations:**
- The precision and recall for class 6 (shirt) have increased. The confusion matrix shows that the model is still not able to differentiate between T-shirt/top and shirts but became better in differentiating shirts with Pullover and Coat.
- The model has become even better at identifying trousers. It has an f1-score of 98% for class 1 (Trousers).
- The overall accuracy on the test data is ~89%, which is approximately the same as the validation accuracy.

Let's visualize the images from the test data.
- We will randomly select 24 images from the test data and visualize them.
- The title of each image would show the actual and predicted label of that image and the probability of the predicted class.
- Higher the probability more confident the model is about the prediction.
"""

rows = 4

cols = 6

fig = plt.figure(figsize = (15, 15))

for i in range(cols):

    for j in range(rows):

        random_index = np.random.randint(0, len(Y_test))

        ax = fig.add_subplot(rows, cols, i * rows + j + 1)

        ax.imshow(X_test[random_index, :])

        pred_label = class_names_list[final_pred[random_index]]

        true_label = class_names_list[Y_test[random_index]]

        y_pred_test_max_probas = np.max(model_3.predict(X_test), axis=1)

        pred_proba = y_pred_test_max_probas[random_index]

        ax.set_title("actual: {}\npredicted: {}\nprobability: {:.3}\n".format(
               true_label, pred_label, pred_proba
        ))

plt.show()

"""## **Conclusion**

- We have trained 3 different models with some changes.
- The plots track the variation in the accuracies and the loss across epochs and allow us to map how better do these models generalize.
- We have observed good performance on the train set but there is some amount of overfitting in the models that get more prominent as we increase the epochs.
- We went ahead with model 3 and evaluated the test data on it. We got an accuracy score of 89% with the classification report describing the class-wise recall and precision.
- Finally, we visualized some of the images from the test data.
"""